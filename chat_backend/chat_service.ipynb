{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "import json\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from pandas import Timestamp\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stt_key1': 'key', 'openai_key1': 'dc0749bc213b4075ab8af52e2bdb442a'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# load azure keys\n",
    "with open('../keys/azure_keys.json') as f:\n",
    "    azure_keys=json.load(f)\n",
    "print(azure_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from openai import AzureOpenAI\n",
    "    \n",
    "# client = AzureOpenAI(\n",
    "#     api_key=azure_keys['openai_key1'],  \n",
    "#     api_version=\"2024-02-01\",\n",
    "#     azure_endpoint = \"https://genai-warriors-openai-test.openai.azure.com/\"\n",
    "#     )\n",
    "    \n",
    "# deployment_name='red-gpt' #This will correspond to the custom name you chose for your deployment when you deployed a model. Use a gpt-35-turbo-instruct deployment. \n",
    "    \n",
    "# # Send a completion call to generate an answer\n",
    "# # print('Sending a test completion job')\n",
    "# start_phrase = 'Write a tagline for an ice cream shop. '\n",
    "# response = client.completions.create(model=deployment_name, prompt=start_phrase, max_tokens=10)\n",
    "# # print(start_phrase+response.choices[0].text)\n",
    "# res=response.choices[0].text\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(context,question):\n",
    "    print(\"-------------generating prediction-----------------\")\n",
    "    # data = json.loads(request.data)\n",
    "    # data.context=data.context.replace('\\n','\\\\n')\n",
    "    # data.context=data.context.replace('\\t','\\\\t')\n",
    "\n",
    "    # context=request.args.get('context')\n",
    "    # question=request.args.get('question')\n",
    "    context=context.replace('\\n','\\\\n')\n",
    "    context=context.replace('\\t','\\\\t')\n",
    "\n",
    "\n",
    "\n",
    "    input=f'paragraph: {context}'\n",
    "    instruction=f'Answer the following question from the meeting transcript: Question : {question}'\n",
    "    # prompts=[[input,instruction]]\n",
    "    prompt=f\"\"\"\n",
    "    Answer the question from the meeting transcript in key points :\n",
    "  \n",
    "    #transcript start\n",
    "    {context}\n",
    "    #transcript end\n",
    "\n",
    "    #question start\n",
    "    {question}\n",
    "    #question end\n",
    "\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    import os\n",
    "    from openai import AzureOpenAI\n",
    "    \n",
    "    client = AzureOpenAI(\n",
    "        api_key=azure_keys['openai_key1'],  \n",
    "        api_version=\"2024-02-01\",\n",
    "        azure_endpoint = \"https://genai-warriors-openai-test.openai.azure.com/\"\n",
    "        )\n",
    "        \n",
    "    deployment_name='red-gpt' #This will correspond to the custom name you chose for your deployment when you deployed a model. Use a gpt-35-turbo-instruct deployment. \n",
    "        \n",
    "    # Send a completion call to generate an answer\n",
    "    # print('Sending a test completion job')\n",
    "    # start_phrase = 'Write a tagline for an ice cream shop. '\n",
    "    response = client.completions.create(model=deployment_name, prompt=prompt, max_tokens=80)\n",
    "    # print(start_phrase+response.choices[0].text)\n",
    "\n",
    "\n",
    "\n",
    "    pt=time.time() - start_time\n",
    "    print(f\"--------------time taken by normal model={pt} seconds\")\n",
    "    res=response.choices[0].text\n",
    "    print(res)\n",
    "\n",
    "    return  { 'output': f'{res}' }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def listen_msgs():\n",
    "    coll_name='sdlc_chat'\n",
    "    user_uid='sdlc_chat_user'\n",
    "        \n",
    "    # Use a service account.\n",
    "    cred = credentials.Certificate(f'../keys/sa.json')\n",
    "\n",
    "    if not firebase_admin._apps:\n",
    "       app = firebase_admin.initialize_app(cred)\n",
    "\n",
    "    db = firestore.client()\n",
    "    \n",
    "    # Create an Event for notifying main thread.\n",
    "    callback_done = threading.Event()\n",
    "    global pdocid\n",
    "    pdocid='000'\n",
    "    # Create a callback on_snapshot function to capture changes\n",
    "    def on_snapshot(doc_snapshot, changes, read_time) :\n",
    "        global pdocid\n",
    "        for doc in doc_snapshot:\n",
    "            data=doc.to_dict()\n",
    "\n",
    "            cdocid=data.get('timestamp').timestamp()\n",
    "            if pdocid==cdocid or pdocid=='000':\n",
    "                print(\"duplicate snapshot---------------------\")\n",
    "                pdocid=cdocid\n",
    "                return\n",
    "            else:\n",
    "                print(\"new doc rec------------\",cdocid,pdocid,type(cdocid),sep='\\n')\n",
    "                pdocid=cdocid\n",
    "\n",
    "\n",
    "\n",
    "            print(f\"Received document snapshot: {doc.id}\")\n",
    "            print(doc.to_dict())\n",
    "\n",
    "            pred_res=predict(data['context'],data['message'])\n",
    "            # pred_res={'output':\"model response\"}\n",
    "\n",
    "            data['senderId']='chatbot@red'\n",
    "            data['message']=pred_res['output']\n",
    "            data['timestamp']=datetime.datetime.utcnow()\n",
    "            print(f'sending response: '+data['message'])\n",
    "            doc_id=str(round(time.time() * 1000))\n",
    "            db.collection(coll_name).document(user_uid).collection('allMessages').document(doc_id).set(data)\n",
    "            print(f\"----------sent----------------with id: {doc_id} \")\n",
    "\n",
    "        callback_done.set()\n",
    "\n",
    "    doc_ref = db.collection(coll_name).document(user_uid).collection('userMessages').document(\"message\")\n",
    "\n",
    "    # Watch the document\n",
    "    doc_watch = doc_ref.on_snapshot(on_snapshot)\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        print('', end='', flush=True)\n",
    "        time.sleep(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listening for messages\n",
      "duplicate snapshot---------------------\n",
      "new doc rec------------\n",
      "1715676729.575\n",
      "1715676662.997\n",
      "<class 'float'>\n",
      "Received document snapshot: message\n",
      "{'senderId': 'sdlc_chat_user@red', 'rType': '1', 'userNotificationToken': '', 'message': 'what are the deadlines set in the meeting?', 'timestamp': DatetimeWithNanoseconds(2024, 5, 14, 8, 52, 9, 575000, tzinfo=datetime.timezone.utc), 'context': 'At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic, human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition: monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three, there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak, hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.'}\n",
      "-------------generating prediction-----------------\n",
      "duplicate snapshot---------------------\n",
      "duplicate snapshot---------------------\n",
      "--------------time taken by normal model=1.6251883506774902 seconds\n",
      "- The meeting discussed a shared goal of achieving a leap in AI capabilities, specifically in multi-sensory and multilingual learning.\n",
      "    - The team hopes to achieve this goal through joint representation, known as the XYZ-code, at the intersection of monolingual text, audio or visual sensory signals, and multilingual.\n",
      "    - They aim to have pre-trained models that can jointly learn representations to support\n",
      "sending response: - The meeting discussed a shared goal of achieving a leap in AI capabilities, specifically in multi-sensory and multilingual learning.\n",
      "    - The team hopes to achieve this goal through joint representation, known as the XYZ-code, at the intersection of monolingual text, audio or visual sensory signals, and multilingual.\n",
      "    - They aim to have pre-trained models that can jointly learn representations to support\n",
      "----------sent----------------with id: 1715676734480 \n",
      "new doc rec------------\n",
      "1715676749.966\n",
      "1715676729.575\n",
      "<class 'float'>\n",
      "Received document snapshot: message\n",
      "{'userNotificationToken': '', 'rType': '1', 'senderId': 'sdlc_chat_user@red', 'message': 'summarize it in 10 words', 'timestamp': DatetimeWithNanoseconds(2024, 5, 14, 8, 52, 29, 966000, tzinfo=datetime.timezone.utc), 'context': 'At Cognizant, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic, human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Services, I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition: monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three, there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak, hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.'}\n",
      "-------------generating prediction-----------------\n",
      "duplicate snapshot---------------------\n",
      "duplicate snapshot---------------------\n",
      "--------------time taken by normal model=1.1257188320159912 seconds\n",
      "\n",
      "AI quest for human-centric machine learning. XYZ-code for joint multilingual learning.\n",
      "sending response: \n",
      "AI quest for human-centric machine learning. XYZ-code for joint multilingual learning.\n",
      "----------sent----------------with id: 1715676752999 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlistening for messages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mlisten_msgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror occured:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \n",
      "Cell \u001b[0;32mIn[17], line 58\u001b[0m, in \u001b[0;36mlisten_msgs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"listening for messages\")\n",
    "    listen_msgs()\n",
    "except Exception as e:\n",
    "    print(f'error occured:\\n\\n\\n {e}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
